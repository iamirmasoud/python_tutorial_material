{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MNIST dataset in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 271s 24us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation=\"relu\", input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The compilation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re now ready to train the network, which in Keras is done via a call to the network’s fit method—we fit the model to its training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2551 - acc: 0.9262\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1036 - acc: 0.9694\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.0677 - acc: 0.9800\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 2s 40us/step - loss: 0.0492 - acc: 0.9853\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.0373 - acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c65d22d3c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s check that the model performs well on the test set, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 79us/step\n",
      "test_acc: 0.9785\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"test_acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAADcRJREFUeJzt3X+o1fUdx/HXu5YRd0k5b2Jmu5NkIMUcHHQwW46WtjBsQaKUGFx0f7hRULSwYhIVNWZRNAd3S2e1pcFW+kfMTEa3wRBP4UrXtiyupJn32g/monK29/44X8et7vl8T+d8z/kefT8fcDnnfN/f7/m+OfXye875fM/3Y+4uAPGcUnYDAMpB+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBPWlTu5s4sSJ3tfX18ldAqEMDQ3p8OHD1si6LYXfzC6X9KCkUyX92t3vTa3f19enarXayi4BJFQqlYbXbfptv5mdKukXkr4vaYakJWY2o9nnA9BZrXzmnyVpr7u/4e5HJW2UtLCYtgC0WyvhnyLpzVGP92fLPsXMVphZ1cyqIyMjLewOQJHa/m2/uw+4e8XdK729ve3eHYAGtRL+A5Kmjnp8XrYMwAmglfDvlDTdzL5mZuMkLZa0pZi2ALRb00N97n7MzH4kaatqQ33r3H1PYZ0BaKuWxvnd/RlJzxTUC4AO4vReICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmppll4zG5J0RNInko65e6WIpgC0X0vhz3zX3Q8X8DwAOoi3/UBQrYbfJT1nZi+a2YoiGgLQGa2+7Z/j7gfM7BxJ28zs7+4+OHqF7B+FFZJ0/vnnt7g7AEVp6cjv7gey22FJT0maNcY6A+5ecfdKb29vK7sDUKCmw29mPWZ25vH7kuZJ2l1UYwDaq5W3/ZMkPWVmx5/nd+7+x0K6AtB2TYff3d+Q9I0CewHQQQz1AUERfiAowg8ERfiBoAg/EBThB4Iq4ld96GI7duxI1h977LFkfXBwMFnfvbv587rWrFmTrJ977rnJ+gsvvJCsL126tG5t9uzZyW0j4MgPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzn8S2LRpU93aDTfckNx2ZGQkWXf3ZH3u3LnJ+uHD9S/sfPPNNye3zZPXW2rfGzdubGnfJwOO/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOP8XeDYsWPJ+s6dO5P15cuX16198MEHyW0vueSSZP2OO+5I1ufMmZOsf/zxx3VrixYtSm67devWZD1PpcKM8Skc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqNxxfjNbJ2mBpGF3vzBbNkHSJkl9koYkLXL399rX5snt8ccfT9b7+/ubfu558+Yl66lrAUjS+PHjm9533vO3Oo4/derUZH3ZsmUtPf/JrpEj/28kXf6ZZbdK2u7u0yVtzx4DOIHkht/dByW9+5nFCyVtyO5vkHRVwX0BaLNmP/NPcveD2f23JU0qqB8AHdLyF35eu5Ba3YupmdkKM6uaWTXvenEAOqfZ8B8ys8mSlN0O11vR3QfcveLuld7e3iZ3B6BozYZ/i6TjX6Uuk7S5mHYAdEpu+M3sCUl/kfR1M9tvZv2S7pV0mZm9Jul72WMAJ5DccX53X1KndGnBvZy0br/99mT9nnvuSdbNLFlfuXJl3dpdd92V3LbVcfw8d999d9ue+6GHHkrW+ZiZxhl+QFCEHwiK8ANBEX4gKMIPBEX4gaC4dHcB7rzzzmQ9byjv9NNPT9bnz5+frN933311a2eccUZy2zwfffRRsv7ss88m6/v27atby5tiO++y4QsXLkzWkcaRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCYpy/Qe+//37d2tq1a5Pb5v0kN28c/+mnn07WW7F3795k/dprr03Wq9Vq0/u+5pprkvVbbrml6edGPo78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4/wNOnr0aN1aq9OQ5V2Ceni47oRIkqT169fXrW3enJ5PZc+ePcn6kSNHkvW8cxhOOaX+8eW6665LbtvT05OsozUc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqNxxfjNbJ2mBpGF3vzBbtlrScknHB7hXufsz7WqyG4wbN65u7ZxzzklumzdO39fXl6znjaW3YsqUKcl63hTeb731VrI+ceLEurUrr7wyuS3aq5Ej/28kXT7G8gfcfWb2d1IHHzgZ5Ybf3QclvduBXgB0UCuf+X9sZi+b2TozO7uwjgB0RLPh/6WkaZJmSjooaU29Fc1shZlVzaza6jnwAIrTVPjd/ZC7f+Lu/5X0K0mzEusOuHvF3Su9vb3N9gmgYE2F38wmj3r4A0m7i2kHQKc0MtT3hKS5kiaa2X5JP5U018xmSnJJQ5J+2MYeAbRBbvjdfckYix9pQy9d7ayzzqpby7uu/oIFC5L1d955J1m/4IILkvXUPPXXX399ctsJEyYk64sXL07W88b587ZHeTjDDwiK8ANBEX4gKMIPBEX4gaAIPxAUl+4uwOzZs5P1bj6teXBwMFl//vnnk/W8nxtPmzbtC/eEzuDIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc4f3Icffpis543j59X5SW/34sgPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzh/c/Pnzy24BJeHIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANB5Y7zm9lUSY9KmiTJJQ24+4NmNkHSJkl9koYkLXL399rXKtph69atZbeAkjRy5D8m6SZ3nyHpW5JWmtkMSbdK2u7u0yVtzx4DOEHkht/dD7r7S9n9I5JelTRF0kJJG7LVNki6ql1NAijeF/rMb2Z9kr4paYekSe5+MCu9rdrHAgAniIbDb2ZflvR7STe6+79G19zdVfs+YKztVphZ1cyq3TxnHRBNQ+E3s9NUC/5v3f0P2eJDZjY5q0+WNDzWtu4+4O4Vd6/09vYW0TOAAuSG32qXZ31E0qvufv+o0hZJy7L7yyRtLr49AO3SyE96vy1pqaRXzGxXtmyVpHslPWlm/ZL2SVrUnhbRTq+//nrZLaAkueF39z9Lqndx9kuLbQdAp3CGHxAU4QeCIvxAUIQfCIrwA0ERfiAoLt0d3MUXX5ys187cxsmIIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBMU4f3AXXXRRsj59+vRkPe96AKk6V3YqF0d+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiKcX4krVq1Klnv7+9vevuHH344ue2MGTOSdbSGIz8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBJU7zm9mUyU9KmmSJJc04O4PmtlqScsljWSrrnL3Z9rVKMpx9dVXJ+sbN25M1rdt21a3tnr16uS269evT9Z7enqSdaQ1cpLPMUk3uftLZnampBfN7Ph/0Qfc/eftaw9Au+SG390PSjqY3T9iZq9KmtLuxgC01xf6zG9mfZK+KWlHtujHZvayma0zs7PrbLPCzKpmVh0ZGRlrFQAlaDj8ZvZlSb+XdKO7/0vSLyVNkzRTtXcGa8bazt0H3L3i7hWu2QZ0j4bCb2anqRb837r7HyTJ3Q+5+yfu/l9Jv5I0q31tAihabvjNzCQ9IulVd79/1PLJo1b7gaTdxbcHoF0a+bb/25KWSnrFzHZly1ZJWmJmM1Ub/huS9MO2dIhSjR8/Pll/8sknk/Xbbrutbm3t2rXJbfOGAvnJb2sa+bb/z5JsjBJj+sAJjDP8gKAIPxAU4QeCIvxAUIQfCIrwA0GZu3dsZ5VKxavVasf2B0RTqVRUrVbHGpr/HI78QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBUR8f5zWxE0r5RiyZKOtyxBr6Ybu2tW/uS6K1ZRfb2VXdv6Hp5HQ3/53ZuVnX3SmkNJHRrb93al0RvzSqrN972A0ERfiCossM/UPL+U7q1t27tS6K3ZpXSW6mf+QGUp+wjP4CSlBJ+M7vczP5hZnvN7NYyeqjHzIbM7BUz22Vmpf7+OJsGbdjMdo9aNsHMtpnZa9ntmNOkldTbajM7kL12u8zsipJ6m2pmfzKzv5nZHjO7IVte6muX6KuU163jb/vN7FRJ/5R0maT9knZKWuLuf+toI3WY2ZCkiruXPiZsZt+R9G9Jj7r7hdmyn0l6193vzf7hPNvdf9Ilva2W9O+yZ27OJpSZPHpmaUlXSbpeJb52ib4WqYTXrYwj/yxJe939DXc/KmmjpIUl9NH13H1Q0rufWbxQ0obs/gbV/ufpuDq9dQV3P+juL2X3j0g6PrN0qa9doq9SlBH+KZLeHPV4v7prym+X9JyZvWhmK8puZgyTsmnTJeltSZPKbGYMuTM3d9JnZpbumteumRmvi8YXfp83x91nSvq+pJXZ29uu5LXPbN00XNPQzM2dMsbM0v9X5mvX7IzXRSsj/AckTR31+LxsWVdw9wPZ7bCkp9R9sw8fOj5JanY7XHI//9dNMzePNbO0uuC166YZr8sI/05J083sa2Y2TtJiSVtK6ONzzKwn+yJGZtYjaZ66b/bhLZKWZfeXSdpcYi+f0i0zN9ebWVolv3ZdN+O1u3f8T9IVqn3j/7qk28rooU5f0yT9NfvbU3Zvkp5Q7W3gf1T7bqRf0lckbZf0mqTnJE3oot4ek/SKpJdVC9rkknqbo9pb+pcl7cr+rij7tUv0Vcrrxhl+QFB84QcERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+IKj/AWrTQ8hNqS7JAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c65e345dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit.reshape(28, 28), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for the *n*th batch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch = train_images\\[128 \\* n : 128 \\* (n + 1)\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering such a batch tensor, the first axis (axis 0) is called the batch axis or batch dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data you’ll manipulate will almost always fall into one of the following categories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vector data—2D tensors of shape (samples, features)\n",
    "\n",
    "* Timeseries data or sequence data—3D tensors of shape (samples, timesteps, features)\n",
    "\n",
    "* Images—4D tensors of shape (samples, height, width, channels) or (samples, channels, height, width)\n",
    "\n",
    "* Video—5D tensors of shape (samples, frames, height, width, channels) or (samples, frames, channels, height, width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch of data will be encoded as a 2D tensor (that is, an array of vectors), where the first axis is the samples axis and the second axis is the features axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An actuarial dataset of people, where we consider each person’s age, ZIP code,\n",
    "and income. Each person can be characterized as a vector of 3 values, and thus\n",
    "an entire dataset of 100,000 people can be stored in a 2D tensor of shape\n",
    "(100000, 3).\n",
    "\n",
    "A dataset of text documents, where we represent each document by the counts\n",
    "of how many times each word appears in it (out of a dictionary of 20,000 common words). Each document can be encoded as a vector of 20,000 values (one\n",
    "count per word in the dictionary), and thus an entire dataset of 500 documents\n",
    "can be stored in a tensor of shape (500, 20000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeseries data or sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever time matters in your data (or the notion of sequence order), it makes sense\n",
    "to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a\n",
    "sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/series.png \"Time Series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time axis is always the second axis (axis of index 1), by convention. Let’s look at a\n",
    "few examples:\n",
    "\n",
    "* A dataset of stock prices. Every minute, we store the current price of the stock,\n",
    "the highest price in the past minute, and the lowest price in the past minute.\n",
    "Thus every minute is encoded as a 3D vector, an entire day of trading is\n",
    "encoded as a 2D tensor of shape (390, 3) (there are 390 minutes in a trading\n",
    "day), and 250 days’ worth of data can be stored in a 3D tensor of shape (250,\n",
    "390, 3). Here, each sample would be one day’s worth of data.\n",
    "\n",
    "* A dataset of tweets, where we encode each tweet as a sequence of 280 characters\n",
    "out of an alphabet of 128 unique characters. In this setting, each character can\n",
    "be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry\n",
    "at the index corresponding to the character). Then each tweet can be encoded\n",
    "as a 2D tensor of shape (280, 128), and a dataset of 1 million tweets can be\n",
    "stored in a tensor of shape (1000000, 280, 128)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/image.png \"Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images typically have three dimensions: height, width, and color depth. \n",
    "\n",
    "Grayscale images (like our MNIST digits) have only a single color channel and could\n",
    "thus be stored in 2D tensors.\n",
    "\n",
    "By convention image tensors are always 3D, with a one dimensional color channel for grayscale images. A batch of 128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape (128, 256, 256, 1), and a batch of 128 color images could be stored in a tensor of shape (128, 256, 256, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two conventions for shapes of images tensors: \n",
    "\n",
    "*channels-last* convention (used by TensorFlow):\n",
    "places the color-depth axis at the end: (samples, height, width, color_depth)\n",
    "\n",
    "*channels-first* convention (used by Theano): places the color depth axis right after the batch axis: (samples, color_depth, height, width). \n",
    "\n",
    "With the Theano convention, the previous examples would become (128, 1, 256, 256) and (128, 3, 256, 256). The Keras framework provides support for both formats.\n",
    "\n",
    "\n",
    "**The Keras framework provides support for both formats.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video data is one of the few types of real-world data for which you’ll need 5D tensors.\n",
    "A video can be understood as a sequence of frames, each frame being a color image.\n",
    "Because each frame can be stored in a 3D tensor (height, width, color_depth), a\n",
    "sequence of frames can be stored in a 4D tensor (frames, height, width, color_depth), and thus a batch of different videos can be stored in a 5D tensor of shape\n",
    "(samples, frames, height, width, color_depth).\n",
    "\n",
    "\n",
    "For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per\n",
    "second would have 240 frames. A batch of four such video clips would be stored in a\n",
    "tensor of shape (4, 240, 144, 256, 3). That’s a total of 106,168,320 values! If the\n",
    "dtype of the tensor was float32, then each value would be stored in 32 bits, so the\n",
    "tensor would represent 405 MB. Heavy! Videos you encounter in real life are much\n",
    "lighter, because they aren’t stored in float32, and they’re typically compressed by a\n",
    "large factor (such as in the MPEG format)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Axes (called broadcast axes) are added to the smaller tensor to match the ndim of\n",
    "the larger tensor.\n",
    "\n",
    "2 The smaller tensor is repeated alongside these new axes to match the full shape\n",
    "of the larger tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With broadcasting, you can generally apply two-tensor element-wise operations if one\n",
    "tensor has shape (a, b, … n, n + 1, … m) and the other has shape (n, n + 1, … m). The\n",
    "broadcasting will then automatically happen for axes *a* through *n - 1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y = np.random.random((32, 10))\n",
    "z = np.minimum(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 3, 32, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0.75069821,  0.16130942,  0.38884924, ...,  0.15575001,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.55356849,  0.21229857,  0.26908443, ...,  0.6546875 ,\n",
       "           0.40958619,  0.0141628 ],\n",
       "         [ 0.07901873,  0.34900568,  0.23717075, ...,  0.08270249,\n",
       "           0.04828707,  0.66042581],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.00582571,  0.45689244, ...,  0.64181985,\n",
       "           0.09550368,  0.18371791],\n",
       "         [ 0.41151263,  0.77417962,  0.30570454, ...,  0.08403572,\n",
       "           0.02559789,  0.50572916],\n",
       "         [ 0.56267997,  0.07924363,  0.0283467 , ...,  0.51536288,\n",
       "           0.56257893,  0.74142897]],\n",
       "\n",
       "        [[ 0.75069821,  0.49244232,  0.71398308, ...,  0.15575001,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.54610355,  0.26809135,  0.26908443, ...,  0.44582135,\n",
       "           0.2152717 ,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.21720843, ...,  0.08420768,\n",
       "           0.04828707,  0.02501114],\n",
       "         ..., \n",
       "         [ 0.2737136 ,  0.61132965,  0.66497205, ...,  0.42058849,\n",
       "           0.1821242 ,  0.42935992],\n",
       "         [ 0.12871046,  0.15697629,  0.05425397, ...,  0.08403572,\n",
       "           0.0446674 ,  0.1174313 ],\n",
       "         [ 0.56267997,  0.21304553,  0.49846247, ...,  0.51536288,\n",
       "           0.56257893,  0.34281603]],\n",
       "\n",
       "        [[ 0.75069821,  0.49244232,  0.4080183 , ...,  0.15575001,\n",
       "           0.24936032,  0.07875981],\n",
       "         [ 0.2954019 ,  0.26809135,  0.16859234, ...,  0.64460577,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.27240726, ...,  0.28159426,\n",
       "           0.04828707,  0.66042581],\n",
       "         ..., \n",
       "         [ 0.66031588,  0.61132965,  0.45775612, ...,  0.43460637,\n",
       "           0.1821242 ,  0.70262922],\n",
       "         [ 0.41151263,  0.54328503,  0.09479977, ...,  0.08403572,\n",
       "           0.30345991,  0.69328144],\n",
       "         [ 0.56267997,  0.21304553,  0.30137066, ...,  0.29000129,\n",
       "           0.46525984,  0.28259069]]],\n",
       "\n",
       "\n",
       "       [[[ 0.55946461,  0.49244232,  0.82114035, ...,  0.15575001,\n",
       "           0.24936032,  0.36047783],\n",
       "         [ 0.10684766,  0.26809135,  0.26908443, ...,  0.34087072,\n",
       "           0.42186112,  0.10214531],\n",
       "         [ 0.07901873,  0.02292306,  0.810897  , ...,  0.15606589,\n",
       "           0.04828707,  0.04373453],\n",
       "         ..., \n",
       "         [ 0.54006303,  0.02704932,  0.14169145, ...,  0.65868693,\n",
       "           0.1821242 ,  0.70262922],\n",
       "         [ 0.09281292,  0.35568421,  0.82508627, ...,  0.08403572,\n",
       "           0.26372665,  0.38732875],\n",
       "         [ 0.56267997,  0.21304553,  0.55118202, ...,  0.07706438,\n",
       "           0.4419954 ,  0.24245197]],\n",
       "\n",
       "        [[ 0.06610206,  0.49244232,  0.29653644, ...,  0.15575001,\n",
       "           0.23553621,  0.36444384],\n",
       "         [ 0.51677965,  0.26809135,  0.26908443, ...,  0.69052465,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.71399324, ...,  0.25644958,\n",
       "           0.04828707,  0.40639646],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.49742812,  0.69495317, ...,  0.40603909,\n",
       "           0.1821242 ,  0.52213329],\n",
       "         [ 0.41151263,  0.23452736,  0.71090486, ...,  0.08403572,\n",
       "           0.85818655,  0.32282046],\n",
       "         [ 0.56267997,  0.21304553,  0.55118202, ...,  0.51536288,\n",
       "           0.49974274,  0.74142897]],\n",
       "\n",
       "        [[ 0.4378634 ,  0.38982156,  0.66500445, ...,  0.120868  ,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.97849242,  0.06565882,  0.26908443, ...,  0.75051979,\n",
       "           0.06545211,  0.11274713],\n",
       "         [ 0.06419285,  0.16275354,  0.6122282 , ...,  0.28159426,\n",
       "           0.04828707,  0.56117865],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.61132965,  0.51116462, ...,  0.1746629 ,\n",
       "           0.1821242 ,  0.70262922],\n",
       "         [ 0.41151263,  0.66762378,  0.82508627, ...,  0.08403572,\n",
       "           0.3525788 ,  0.23077655],\n",
       "         [ 0.24686856,  0.04019581,  0.51138375, ...,  0.51536288,\n",
       "           0.56257893,  0.1353535 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.59010553,  0.49244232,  0.56482563, ...,  0.15575001,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.97758951,  0.26809135,  0.26908443, ...,  0.21566662,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.77278306, ...,  0.28053198,\n",
       "           0.04828707,  0.16457262],\n",
       "         ..., \n",
       "         [ 0.56532151,  0.61132965,  0.50781126, ...,  0.35778313,\n",
       "           0.1821242 ,  0.27626655],\n",
       "         [ 0.41151263,  0.25797683,  0.61787086, ...,  0.08403572,\n",
       "           0.46483657,  0.28716114],\n",
       "         [ 0.56267997,  0.13914926,  0.00680142, ...,  0.35503486,\n",
       "           0.56257893,  0.48428019]],\n",
       "\n",
       "        [[ 0.30130769,  0.49244232,  0.4654964 , ...,  0.15575001,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.60757956,  0.26809135,  0.08834673, ...,  0.18376158,\n",
       "           0.42186112,  0.02748334],\n",
       "         [ 0.07901873,  0.34900568,  0.25412883, ...,  0.28159426,\n",
       "           0.04828707,  0.19893701],\n",
       "         ..., \n",
       "         [ 0.32117653,  0.26109455,  0.69495317, ...,  0.65868693,\n",
       "           0.1821242 ,  0.70262922],\n",
       "         [ 0.41151263,  0.73869948,  0.49481702, ...,  0.08403572,\n",
       "           0.02862756,  0.6686319 ],\n",
       "         [ 0.45766342,  0.21304553,  0.21443004, ...,  0.44576075,\n",
       "           0.02894908,  0.60308607]],\n",
       "\n",
       "        [[ 0.28132473,  0.38934372,  0.02680664, ...,  0.15575001,\n",
       "           0.23568862,  0.37083805],\n",
       "         [ 0.29931105,  0.2611164 ,  0.26908443, ...,  0.22975883,\n",
       "           0.09051007,  0.11274713],\n",
       "         [ 0.07901873,  0.32528161,  0.52716671, ...,  0.28159426,\n",
       "           0.04828707,  0.66042581],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.34889002,  0.69495317, ...,  0.65868693,\n",
       "           0.1821242 ,  0.09005993],\n",
       "         [ 0.33459276,  0.0210033 ,  0.38609488, ...,  0.08403572,\n",
       "           0.14841821,  0.69328144],\n",
       "         [ 0.56267997,  0.21304553,  0.19088146, ...,  0.3423735 ,\n",
       "           0.10950784,  0.58758124]]],\n",
       "\n",
       "\n",
       "       ..., \n",
       "       [[[ 0.49310056,  0.2139587 ,  0.80642777, ...,  0.15575001,\n",
       "           0.02540865,  0.37083805],\n",
       "         [ 0.60184812,  0.26809135,  0.03228787, ...,  0.66698225,\n",
       "           0.00766361,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.48307133, ...,  0.28159426,\n",
       "           0.04828707,  0.20947174],\n",
       "         ..., \n",
       "         [ 0.25651417,  0.27383785,  0.69495317, ...,  0.55621017,\n",
       "           0.1821242 ,  0.53435916],\n",
       "         [ 0.38097355,  0.11444055,  0.5365851 , ...,  0.08403572,\n",
       "           0.85818655,  0.69328144],\n",
       "         [ 0.4538094 ,  0.21304553,  0.55118202, ...,  0.26320214,\n",
       "           0.56257893,  0.01707467]],\n",
       "\n",
       "        [[ 0.4877926 ,  0.49244232,  0.65714254, ...,  0.15575001,\n",
       "           0.19073411,  0.34086079],\n",
       "         [ 0.45129638,  0.26809135,  0.26908443, ...,  0.61504311,\n",
       "           0.40650029,  0.11274713],\n",
       "         [ 0.07901873,  0.04244053,  0.32315864, ...,  0.28159426,\n",
       "           0.04828707,  0.62029471],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.61132965,  0.47949378, ...,  0.65868693,\n",
       "           0.10126128,  0.70262922],\n",
       "         [ 0.41151263,  0.68228227,  0.7667778 , ...,  0.08403572,\n",
       "           0.51384287,  0.57501747],\n",
       "         [ 0.56267997,  0.21304553,  0.55118202, ...,  0.51536288,\n",
       "           0.45817106,  0.36135031]],\n",
       "\n",
       "        [[ 0.42883178,  0.49244232,  0.37286487, ...,  0.15575001,\n",
       "           0.24936032,  0.23759433],\n",
       "         [ 0.69586688,  0.26809135,  0.26908443, ...,  0.05632885,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.19448202,  0.72031506, ...,  0.28159426,\n",
       "           0.04828707,  0.28129294],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.61132965,  0.69495317, ...,  0.09330002,\n",
       "           0.1821242 ,  0.36829344],\n",
       "         [ 0.12683797,  0.29519028,  0.82508627, ...,  0.08403572,\n",
       "           0.64764986,  0.69328144],\n",
       "         [ 0.56267997,  0.21304553,  0.11184595, ...,  0.51536288,\n",
       "           0.32723089,  0.03731333]]],\n",
       "\n",
       "\n",
       "       [[[ 0.46656164,  0.18166204,  0.81143747, ...,  0.12241519,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.1152065 ,  0.26809135,  0.26908443, ...,  0.75051979,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.15656282,  0.80600527, ...,  0.28159426,\n",
       "           0.04828707,  0.20856654],\n",
       "         ..., \n",
       "         [ 0.52277503,  0.39697612,  0.03334395, ...,  0.16179678,\n",
       "           0.1821242 ,  0.23546721],\n",
       "         [ 0.41151263,  0.02995788,  0.39818217, ...,  0.08403572,\n",
       "           0.26968833,  0.69328144],\n",
       "         [ 0.56267997,  0.21304553,  0.25520192, ...,  0.42892184,\n",
       "           0.42473917,  0.27302403]],\n",
       "\n",
       "        [[ 0.32435135,  0.04768141,  0.22104863, ...,  0.15575001,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.80996781,  0.26809135,  0.12539719, ...,  0.50127912,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.89217508, ...,  0.28159426,\n",
       "           0.04828707,  0.66042581],\n",
       "         ..., \n",
       "         [ 0.21737827,  0.07976782,  0.69495317, ...,  0.65868693,\n",
       "           0.1821242 ,  0.12857836],\n",
       "         [ 0.41151263,  0.77417962,  0.05735636, ...,  0.04244021,\n",
       "           0.03997727,  0.69328144],\n",
       "         [ 0.23007116,  0.09869014,  0.10554087, ...,  0.04650807,\n",
       "           0.4555568 ,  0.74142897]],\n",
       "\n",
       "        [[ 0.75069821,  0.49244232,  0.32854608, ...,  0.15575001,\n",
       "           0.1928695 ,  0.37083805],\n",
       "         [ 0.43565506,  0.26809135,  0.26908443, ...,  0.66357173,\n",
       "           0.04220841,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.38895129, ...,  0.28159426,\n",
       "           0.04828707,  0.32535014],\n",
       "         ..., \n",
       "         [ 0.38202638,  0.61132965,  0.10755631, ...,  0.21083688,\n",
       "           0.1821242 ,  0.02425948],\n",
       "         [ 0.26792307,  0.35893398,  0.2473311 , ...,  0.08403572,\n",
       "           0.14329672,  0.56167888],\n",
       "         [ 0.50758032,  0.21304553,  0.42988448, ...,  0.30673714,\n",
       "           0.56257893,  0.43366722]]],\n",
       "\n",
       "\n",
       "       [[[ 0.24019125,  0.49244232,  0.17453754, ...,  0.15575001,\n",
       "           0.24936032,  0.07988915],\n",
       "         [ 0.15367406,  0.26809135,  0.26908443, ...,  0.65439757,\n",
       "           0.21541165,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.70754236, ...,  0.28159426,\n",
       "           0.04828707,  0.66042581],\n",
       "         ..., \n",
       "         [ 0.32817774,  0.25332136,  0.69495317, ...,  0.29488214,\n",
       "           0.1821242 ,  0.53937343],\n",
       "         [ 0.39554585,  0.50394301,  0.73687245, ...,  0.08403572,\n",
       "           0.67619988,  0.69328144],\n",
       "         [ 0.08788449,  0.21304553,  0.12151699, ...,  0.51536288,\n",
       "           0.56257893,  0.21017494]],\n",
       "\n",
       "        [[ 0.05877303,  0.08563811,  0.71705335, ...,  0.09407989,\n",
       "           0.24936032,  0.37083805],\n",
       "         [ 0.55198592,  0.26809135,  0.26908443, ...,  0.7009356 ,\n",
       "           0.42186112,  0.11274713],\n",
       "         [ 0.07901873,  0.17781599,  0.82803424, ...,  0.28159426,\n",
       "           0.04828707,  0.50393142],\n",
       "         ..., \n",
       "         [ 0.23966056,  0.60197011,  0.38732792, ...,  0.56014005,\n",
       "           0.1821242 ,  0.70262922],\n",
       "         [ 0.41151263,  0.16166805,  0.30668057, ...,  0.08403572,\n",
       "           0.64948212,  0.69328144],\n",
       "         [ 0.39804413,  0.21304553,  0.55118202, ...,  0.13258396,\n",
       "           0.34372659,  0.30439937]],\n",
       "\n",
       "        [[ 0.19667274,  0.49244232,  0.37737979, ...,  0.15575001,\n",
       "           0.24936032,  0.17064282],\n",
       "         [ 0.69955709,  0.26809135,  0.26908443, ...,  0.19981718,\n",
       "           0.10959883,  0.11274713],\n",
       "         [ 0.07901873,  0.34900568,  0.6280814 , ...,  0.0975853 ,\n",
       "           0.04828707,  0.61645745],\n",
       "         ..., \n",
       "         [ 0.67744607,  0.09895343,  0.2636909 , ...,  0.65868693,\n",
       "           0.1821242 ,  0.26128532],\n",
       "         [ 0.41151263,  0.71365302,  0.13058861, ...,  0.08403572,\n",
       "           0.41206191,  0.69328144],\n",
       "         [ 0.32585777,  0.21304553,  0.46297659, ...,  0.03494906,\n",
       "           0.56257893,  0.30373638]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers: the building blocks of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different layers are appropriate for different tensor formats and different types of data\n",
    "processing. \n",
    "For instance, simple vector data, stored in 2D tensors of shape (samples, features), is often processed by densely connected layers, also called fully connected or dense layers (the Dense class in Keras). \n",
    "\n",
    "Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by recurrent layers such as an LSTM layer.\n",
    "\n",
    "Image data, stored in 4D tensors, is usually processed by 2D convolution layers (Conv2D)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of layer compatibility here refers specifically to the fact that every layer\n",
    "will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "layer = layers.Dense(32, input_shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re creating a layer that will only accept as input 2D tensors where the first dimension is 784 (axis 0, *the batch dimension, is unspecified, and thus any value would be accepted*). This layer will return a tensor where the first dimension has been transformed to be 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Keras, you don’t have to worry about\n",
    "compatibility, because the layers you add to your models are dynamically built to\n",
    "match the shape of the incoming layer. For instance, suppose you write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, input_shape=(784,)))\n",
    "model.add(layers.Dense(32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second layer didn’t receive an input shape argument—instead, it automatically\n",
    "inferred its input shape as being the output shape of the layer that came before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions and optimizers: keys to configuring the learning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function (objective function)—The quantity that will be minimized during training. It represents a measure of success for the task at hand.\n",
    "\n",
    "Optimizer—Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are simple guidelines you can follow to choose the\n",
    "correct loss. \n",
    "\n",
    "You’ll use binary crossentropy for a two-class classification problem \n",
    "\n",
    "categorical crossentropy for a many-class classification problem\n",
    "\n",
    "mean squared error for a regression problem\n",
    "\n",
    "connectionist temporal classification (CTC) for a sequence-learning problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras has the following key features:\n",
    "* It allows the same code to run seamlessly on CPU or GPU.\n",
    "\n",
    "* It has a user-friendly API that makes it easy to quickly prototype deep-learning models.\n",
    "\n",
    "* It has built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.\n",
    "\n",
    "* It supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, and so on. This means Keras is appropriate for building essentially any deep-learning model, from a generative adversarial network to a neural Turing machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a model-level library, providing high-level building blocks for developing deep-learning models. \n",
    "\n",
    "It doesn’t handle low-level operations such as tensor manipulation and differentiation. Instead, it relies on a specialized, well-optimized tensor library to do so, serving as the backend engine of Keras. \n",
    "\n",
    "Rather than choosing a single tensor library and tying the implementation of Keras to that library, Keras handles the problem in a modular way (see figure 3.3); thus several different backend engines can be plugged seamlessly into Keras. \n",
    "\n",
    "Currently, the three existing backend implementations are the TensorFlow backend, the Theano backend, and the Microsoft Cognitive Toolkit (CNTK) backend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any piece of code that you write with Keras can be run with any of these backends without having to change anything in the code: you can seamlessly switch between the two\n",
    "during development.\n",
    "\n",
    "We recommend using the TensorFlow backend as the default for most of your deep-learning needs, because it’s the most widely adopted, scalable, and production ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Via TensorFlow (or Theano, or CNTK), Keras is able to run seamlessly on both CPUs and GPUs. When running on CPU, TensorFlow is itself wrapping a low-level library for tensor operations called Eigen (http://eigen.tuxfamily.org). \n",
    "\n",
    "On GPU, TensorFlow wraps a library of well-optimized deep-learning operations called the NVIDIA CUDA Deep Neural Network library (cuDNN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing with Keras: a quick overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/keras.png \"Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Define your training data: input tensors and target tensors.\n",
    "\n",
    "2 Define a network of layers (or model ) that maps your inputs to your targets.\n",
    "\n",
    "3 Configure the learning process by choosing a loss function, an optimizer, and some metrics to monitor.\n",
    "\n",
    "4 Iterate on your training data by calling the fit() method of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to define a model: \n",
    "*Sequential* class: only for linear stacks of layers, which is the most common network architecture by far\n",
    "\n",
    "*functional API* :for directed acyclic graphs of layers, which lets you build completely arbitrary architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation=\"relu\", input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here’s the same model defined using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = layers.Input(shape=(784,))\n",
    "x = layers.Dense(32, activation=\"relu\")(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = models.Model(inputs=input_tensor, outputs=output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your model architecture is defined, it doesn’t matter whether you used a Sequential model or the functional API. All of the following steps are the same:\n",
    "\n",
    "The learning process is configured in the *compilation step*, where you specify the *optimizer* and *loss function(s)* that the model should use, as well as the metrics you want to monitor during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s an example with a single loss function, which is by far the most common case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=\"mse\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the learning process consists of passing Numpy arrays of input data (and the corresponding target data) to the model via the **fit()** method, similar to what you would do in Scikit-Learn and several other machine-learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-c17b2397d844>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(input_tensor, target_tensor, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a deep-learning workstation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s highly recommended, although not strictly necessary, that you run deep-learning code on a modern NVIDIA GPU. Some applications—in particular, image processing with convolutional networks and sequence processing with recurrent neural networks—will be excruciatingly slow on CPU, even a fast multicore CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even for applications that can realistically be run on CPU, you’ll generally see speed increase by a factor or 5 or 10 by using a modern GPU. If you don’t want to\n",
    "install a GPU on your machine, you can alternatively consider running your experiments on an *AWS EC2 GPU* instance or on *Google Cloud Platform*. But note that cloud GPU instances can become expensive over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s better to be using a Unix workstation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to use Keras, you need to install TensorFlow or CNTK or Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter notebooks: the preferred way to run deep-learning experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code examples in this book are available as open source\n",
    "notebooks; you can download them from the book’s website at \n",
    "\n",
    "www.manning.com/books/deep-learning-with-python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Keras running: two options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the official EC2 Deep Learning AMI (https://aws.amazon.com/amazonai/amis), and run Keras experiments as Jupyter notebooks on EC2. Do this if you don’t already have a GPU on your local machine. Appendix B provides a step-by-step guide.\n",
    "\n",
    "* Install everything from scratch on a local Unix workstation. You can then run either local Jupyter notebooks or a regular Python codebase. Do this if you already have a high-end NVIDIA GPU. Appendix A provides an Ubuntu-specific, step-by-step guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running deep-learning jobs in the cloud: pros and cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of mid-2017, the cloud offering that makes it easiest to get started with deep learning is definitely AWS EC2.\n",
    "\n",
    "EC2 instances are expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the best GPU for deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is that it must be an NVIDIA GPU. NVIDIA is the only graphics computing company that has invested heavily in deep learning so far, and modern deep-learning frameworks can only run on NVIDIA cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of mid-2017, we recommend the NVIDIA TITAN Xp as the best card on the market for deep learning. For lower budgets, you may want to consider the GTX 1060. If\n",
    "you’re reading these pages in 2018 or later, take the time to look online for fresher\n",
    "recommendations, because new models come out every year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Keras and its dependencies on Ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Install the Python scientific suite—Numpy and SciPy—and make sure you have a Basic Linear Algebra Subprogram (BLAS) library installed so your models run fast on CPU.\n",
    "\n",
    "2 Install two extras packages that come in handy when using Keras: HDF5 (for saving large neural-network files) and Graphviz (for visualizing neuralnetwork architectures).\n",
    "\n",
    "3 Make sure your GPU can run deep-learning code, by installing CUDA drivers and cuDNN.\n",
    "\n",
    "4 Install a backend for Keras: TensorFlow, CNTK, or Theano.\n",
    "\n",
    "5 Install Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, make sure you have pip installed and that your package manager is up to date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\$ sudo apt-get update\n",
    "\n",
    "\\$ sudo apt-get upgrade\n",
    "\n",
    "\\$ sudo apt-get install python-pip python-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, Ubuntu uses Python 2 when it installs Python packages such as pythonpip. If you wish to use Python 3 instead, you should use the python3 prefix instead of python. For instance:\n",
    "\n",
    "\\$ sudo apt-get install python3-pip python3-dev\n",
    "\n",
    "When you’re installing packages using pip, keep in mind that by default, it targets Python 2. To target Python 3, you should use pip3:\n",
    "\n",
    "\\$ sudo pip3 install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the Python scientific suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Install a BLAS library (OpenBLAS, in this case), to ensure that you can run fast\n",
    "tensor operations on your CPU:\n",
    "\n",
    "\\$ sudo apt-get install build-essential cmake git unzip \\\\\n",
    "pkg-config libopenblas-dev liblapack-dev\n",
    "\n",
    "\n",
    "2 Install the Python scientific suite: Numpy, SciPy and Matplotlib. This is necessary in order to perform any kind of machine learning or scientific computing in Python, regardless of whether you’re doing deep learning:\n",
    "\n",
    "\\$ sudo apt-get install python-numpy python-scipy python- matplotlib\n",
    "➥python-yaml\n",
    "\n",
    "3 Install HDF5. This library, originally developed by NASA, stores large files of numeric data in an efficient binary format. It will allow you to save your Keras models to disk quickly andefficiently:\n",
    "\n",
    "\\$ sudo apt-get install libhdf5-serial-dev python-h5py\n",
    "\n",
    "4 Install Graphviz and pydot-ng, two packages that will let you visualize Keras models. They aren’t necessary to run Keras, so you could skip this step and install these packages when you need them. Here are the commands:\n",
    "\n",
    "\\$ sudo apt-get install graphviz\n",
    "\n",
    "\\$ sudo pip install pydot-ng\n",
    "\n",
    "5 Install additional packages that are used in some of our code examples:\n",
    "\n",
    "\\$ sudo apt-get install python-opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your NVIDIA GPU for deep learning, you need to install two things:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA**: A set of drivers for your GPU that allows it to run a low-level programming language for parallel computing.\n",
    " \n",
    "**cuDNN**: A library of highly optimized primitives for deep learning. When using cuDNN and running on a GPU, you can typically increase the training speed of your models by 50% to 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow depends on particular versions of CUDA and the cuDNN library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please consult the TensorFlow website for detailed instructions about which versions are currently recommended: www.tensorflow.org/install/install_linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 Download CUDA. For Ubuntu (and other Linux flavors), NVIDIA provides a ready-to-use package that you can download from https://developer.nvidia.com/cuda-downloads:\n",
    "\n",
    "\\$ wget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.0.176-1_amd64.deb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2  Install CUDA. The easiest way to do so is to use Ubuntu’s apt on this package.\n",
    "This will allow you to easily install updates via apt as they become available:\n",
    "\n",
    "\\$ sudo dpkg -i cuda-repo-ubuntu1604_9.0.176-1_amd64.deb\n",
    "\n",
    "\\$ sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pub\n",
    "\n",
    "\\$ sudo apt-get update\n",
    "\n",
    "\\$ sudo apt-get install cuda-8-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 Install cuDNN:\n",
    "\n",
    "a) \n",
    "Register for a free NVIDIA developer account (unfortunately, this is necessary\n",
    "in order to gain access to the cuDNN download), and download cuDNN at\n",
    "https://developer.NVIDIA.com/cudnn (select the version of cuDNN compatible with TensorFlow). Like CUDA, NVIDIA provides packages for different Linux flavors—we’ll use the version for Ubuntu 16.04. Note that if you’re working with an EC2 install, you won’t be able to download the cuDNN archive directly to your instance; instead, download it to your local machine\n",
    "and then upload it to your EC2 instance (via *scp*).\n",
    "\n",
    "b) Install cuDNN:\n",
    "\n",
    "\\$ sudo dpkg -i dpkg -i libcudnn6*.deb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Install TensorFlow:\n",
    "a) TensorFlow with or without GPU support can be installed from PyPI using\n",
    "Pip. Here’s the command without GPU support:\n",
    "\n",
    "\\$ sudo pip install tensorflow\n",
    "\n",
    "b) Here’s the command to install TensorFlow with GPU support:\n",
    "\n",
    "\\$ sudo pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Theano (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theano can also be installed from PyPI:\n",
    "\n",
    "\\$ sudo pip install theano\n",
    "\n",
    "If you’re using a GPU, then you should configure Theano to use your GPU. You can create a Theano configuration file with this command:\n",
    "\n",
    "nano ~/.theanorc\n",
    "\n",
    "Then, fill in the file with the following configuration:\n",
    "\n",
    "[global]\n",
    "\n",
    "floatX = float32\n",
    "\n",
    "device = gpu0\n",
    "\n",
    "\n",
    "\n",
    "[nvcc]\n",
    "\n",
    "fastmath = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can install Keras from PyPI:\n",
    "\\$ sudo pip install keras\n",
    "\n",
    "Alternatively, you can install Keras from GitHub. Doing so will allow you to access the keras/examples folder, which contains many example scripts for you to learn from:\n",
    "\n",
    "\\$ git clone https://github.com/fchollet/keras\n",
    "\n",
    "\\$ cd keras\n",
    "\n",
    "\\$ sudo python setup.py install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now try to run a Keras script, such as this MNIST example:\n",
    "\n",
    "python examples/mnist_cnn.py\n",
    "\n",
    "Note that running this example to completion may take a few minutes, so feel free to force-quit it (Ctrl-C) once you’ve verified that it’s working normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you’ve run Keras at least once, the Keras configuration file can be found at\n",
    "~/.keras/keras.json. \n",
    "You can edit it to select the backend that Keras runs on: tensorflow, theano, or cntk. Your configuration file should like this:\n",
    "\n",
    "{\n",
    "\"image_data_format\": \"channels_last\",\n",
    "\n",
    "\"epsilon\": 1e-07,\n",
    "\n",
    "\"floatx\": \"float32\",\n",
    "\n",
    "\"backend\": \"tensorflow\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Keras script examples/mnist_cnn.py is running, you can monitor GPU utilization in a different shell window:\n",
    "\n",
    "\\$ watch -n 5 NVIDIA-smi -a --display=utilization\n",
    "\n",
    "You’re all set! Congratulations—you can now begin building deep-learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
